{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlpmd = spacy.load('en_core_web_md')\n",
    "nlplg = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the email modules we'll need\n",
    "import glob\n",
    "import email\n",
    "import mailparser\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "path = '../datawe/raw/Email_Classification/*'\n",
    "email_types = glob.glob(path)\n",
    "appendFilesData = []\n",
    "file_raw_data = [] \n",
    "for folder in email_types:\n",
    "    files = glob.glob(folder+\"/*.txt\")\n",
    "    email_type = folder.split('\\\\')[1]\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name) as fp:\n",
    "                raw_data = fp.read()\n",
    "                file_raw_data.append(raw_data)\n",
    "                msg = mailparser.parse_from_string(raw_data)\n",
    "                appendFilesData.append({\n",
    "                    \"to\":msg.to,\n",
    "                    \"from\":msg.from_,\n",
    "                    \"subject\":msg.subject,\n",
    "                    \"date\":msg.date,\n",
    "#                     \"sent\":msg[\"Sent\"],\n",
    "#                     \"importance\":msg[\"Importance\"],\n",
    "                    \"content\":  msg.body,\n",
    "                    \"class_to_exec\":email_type,\n",
    "                })\n",
    "         \n",
    "        except IOError as exc:\n",
    "            print('Exception')\n",
    "\n",
    "            \n",
    "#creating pandas dataframe\n",
    "data = pd.DataFrame(appendFilesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = data[\"content\"][:1]\n",
    "entityData = []\n",
    "for cnt in body:\n",
    "    entityCh = []\n",
    "    entity = nlp(cnt)\n",
    "    for token in entity.ents:\n",
    "        entityData.append({\"text\":token.text, \"label\":token.label_})\n",
    "#     entityData.append(entityCh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityDF = pd.DataFrame(entityData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CARDINAL    2\n",
       "PERSON      2\n",
       "DATE        2\n",
       "ORG         1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityDF[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entityDF[\"text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = nlp('. '.join(np.array(data[\"content\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in text:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#           token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entityDtl = []\n",
    "# for token in text.ents:\n",
    "#     if len(token.text.rstrip()) > 0:\n",
    "#         entityDtl.append({\"label\":token.label_, \"text\":token.text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entyDF = pd.DataFrame(entityDtl)\n",
    "# entyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(entyDF[\"text\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Dear Sir/ Madam,\n",
    "\n",
    " \n",
    "\n",
    "Greetings of the day!\n",
    "\n",
    " \n",
    "\n",
    "As per your request, I am pleased to assist you with the reservation\n",
    "details as follows:\n",
    "\n",
    " \n",
    "\n",
    "*         Reservation #: 75677SB034081\n",
    "\n",
    "*         Guest Name: Saroj Kumar\n",
    "\n",
    "*         Hotel: Taj Dubai\n",
    "\n",
    "*         Dates of Stay: Tue Nov 20 - Sun Nov 25, 2018\n",
    "\n",
    "*         Room: Luxury Burj View Room King Bed\n",
    "\n",
    "*         Rate: Stay a Bit Longer â€“ AED 6500.61\n",
    "\n",
    "*         Inclusions: Breakfast, complimentary shuttle to Dubai mall at\n",
    "fixed timings and Wi-Fi upto 4 devices \n",
    "\n",
    "*         Cancellation: Reservations must be cancelled by 2PM - 1 day\n",
    "prior to arrival to avoid a penalty of 1 night charge plus taxes\n",
    "\n",
    " \n",
    "\n",
    "Please do not hesitate to contact us if we may be of further assistance\n",
    "to you.\n",
    "\n",
    " \n",
    "\n",
    "Warm Regards,\n",
    "\n",
    "Diana Chettiar\n",
    "\n",
    "Taj Reservations Worldwide Team\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # en_core_web_sm = spacy.load('en_core_web_sm')\n",
    "# # en_core_web_md = spacy.load('en_core_web_md')\n",
    "# # en_core_web_lg = spacy.load('en_core_web_lg')\n",
    "# # en_vectors_web_lg = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "\n",
    "\n",
    "# # en_core_web_sm_doc = en_core_web_sm(sentence)\n",
    "# # en_core_web_md_doc = en_core_web_md(sentence)\n",
    "# # en_core_web_lg_doc = en_core_web_lg(sentence)\n",
    "# # en_vectors_web_lg_doc = en_vectors_web_lg(sentence)\n",
    "\n",
    "# doc = nlp(sentence)\n",
    "\n",
    "# entityDtl = []\n",
    "# for token in text.ents:\n",
    "#     if len(token.text.rstrip()) > 0:\n",
    "#         entityDtl.append({\"label\":token.label_, \"text\":token.text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(entityDtl)\n",
    "\n",
    "entities = {\"GPE\":\"Location\",\n",
    " \"ORG\":\"organization\",\n",
    " \"PERSON\":\"Name\",\n",
    " \"DATE\":\"Date\",\n",
    " \"TIME\" : \"Time\",\n",
    " \"CARDINAL\": \"CARDINAL\"\n",
    "}\n",
    "stemmer = SnowballStemmer(\"english\").stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEntities(text):\n",
    "    entityDtl = []\n",
    "    for token in text.ents:\n",
    "        wrd = token.text.strip()\n",
    "        label_ =  token.label_\n",
    "        if len(wrd) > 0:\n",
    "            entityDtl.append({\"label\":token.label_, \"text\":token.text.strip()})\n",
    "\n",
    "#     if len(wrd) > 0 and label_ in entities and ((label_ == \"PERSON\" and wrd.replace(' ','').isalpha() and stemmer(wrd) == wrd.lower()) or label_ != \"PERSON\"):\n",
    "#         entityDtl.append({\"label\":token.label_, \"text\":token.text.strip()})\n",
    "\n",
    "    return pd.DataFrame(entityDtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlg = nlplg(sentence)\n",
    "textmd = nlpmd(sentence)\n",
    "entityLg = findEntities(textlg)\n",
    "entityMd = findEntities(textmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n\\n', 5), ('75677SB034081', 1), ('Kumar\\n\\n', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# names = entityDtlDF[entityDtlDF[\"label\"] == \"PERSON\"][\"text\"]\n",
    "\n",
    "# # names.\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\").stem\n",
    "# stem_free = [stemmer(stem) for stem in names if stem]\n",
    "# stem_free\n",
    "items = [x.text for x in text.ents]\n",
    "Counter(items).most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       label                             text\n",
       " 0   CARDINAL                    75677SB034081\n",
       " 1     PERSON                            Kumar\n",
       " 2        GPE                        Taj Dubai\n",
       " 3       DATE         Tue Nov 20 - Sun Nov 25,\n",
       " 4       DATE                             2018\n",
       " 5        FAC            Luxury Burj View Room\n",
       " 6        ORG            Inclusions: Breakfast\n",
       " 7        GPE                            Dubai\n",
       " 8        FAC                            Wi-Fi\n",
       " 9       DATE                      2PM - 1 day\n",
       " 10      TIME                          1 night\n",
       " 11    PERSON                   Diana Chettiar\n",
       " 12       ORG  Taj Reservations Worldwide Team,\n",
       "        label                                               text\n",
       " 0     PERSON                                         Sir/ Madam\n",
       " 1   CARDINAL                                      75677SB034081\n",
       " 2     PERSON                                              Guest\n",
       " 3     PERSON                                        Saroj Kumar\n",
       " 4     PERSON                                          Taj Dubai\n",
       " 5       DATE                           Tue Nov 20 - Sun Nov 25,\n",
       " 6       DATE                                               2018\n",
       " 7        FAC                         Luxury Burj View Room King\n",
       " 8   CARDINAL                                            6500.61\n",
       " 9     PERSON                                         Inclusions\n",
       " 10       GPE                                              Dubai\n",
       " 11       FAC                                       Wi-Fi upto 4\n",
       " 12      DATE                                        2PM - 1 day\n",
       " 13      TIME                                            1 night\n",
       " 14    PERSON  Diana Chettiar\\n\\nTaj Reservations Worldwide Team)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityLg, entityMd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((label_ == \"PERSON\" and wrd.isalpha() and stemmer(wrd) == wrd) or label_ != \"PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(wrd) == wrd.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diana chettiar'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.53906965\n",
      "dog banana 0.28761008\n",
      "cat dog 0.53906965\n",
      "cat cat 1.0\n",
      "cat banana 0.4875216\n",
      "banana dog 0.28761008\n",
      "banana cat 0.4875216\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class str in module builtins:\n",
      "\n",
      "class str(object)\n",
      " |  str(object='') -> str\n",
      " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      " |  \n",
      " |  Create a new string object from the given object. If encoding or\n",
      " |  errors is specified, then the object must expose a data buffer\n",
      " |  that will be decoded using the given encoding and error handler.\n",
      " |  Otherwise, returns the result of object.__str__() (if defined)\n",
      " |  or repr(object).\n",
      " |  encoding defaults to sys.getdefaultencoding().\n",
      " |  errors defaults to 'strict'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(...)\n",
      " |      S.__format__(format_spec) -> str\n",
      " |      \n",
      " |      Return a formatted version of S as described by format_spec.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      S.__sizeof__() -> size of S in memory, in bytes\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  capitalize(...)\n",
      " |      S.capitalize() -> str\n",
      " |      \n",
      " |      Return a capitalized version of S, i.e. make the first character\n",
      " |      have upper case and the rest lower case.\n",
      " |  \n",
      " |  casefold(...)\n",
      " |      S.casefold() -> str\n",
      " |      \n",
      " |      Return a version of S suitable for caseless comparisons.\n",
      " |  \n",
      " |  center(...)\n",
      " |      S.center(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S centered in a string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space)\n",
      " |  \n",
      " |  count(...)\n",
      " |      S.count(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the number of non-overlapping occurrences of substring sub in\n",
      " |      string S[start:end].  Optional arguments start and end are\n",
      " |      interpreted as in slice notation.\n",
      " |  \n",
      " |  encode(...)\n",
      " |      S.encode(encoding='utf-8', errors='strict') -> bytes\n",
      " |      \n",
      " |      Encode S using the codec registered for encoding. Default encoding\n",
      " |      is 'utf-8'. errors may be given to set a different error\n",
      " |      handling scheme. Default is 'strict' meaning that encoding errors raise\n",
      " |      a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and\n",
      " |      'xmlcharrefreplace' as well as any other name registered with\n",
      " |      codecs.register_error that can handle UnicodeEncodeErrors.\n",
      " |  \n",
      " |  endswith(...)\n",
      " |      S.endswith(suffix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S ends with the specified suffix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      suffix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  expandtabs(...)\n",
      " |      S.expandtabs(tabsize=8) -> str\n",
      " |      \n",
      " |      Return a copy of S where all tab characters are expanded using spaces.\n",
      " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      " |  \n",
      " |  find(...)\n",
      " |      S.find(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  format(...)\n",
      " |      S.format(*args, **kwargs) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  format_map(...)\n",
      " |      S.format_map(mapping) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from mapping.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  index(...)\n",
      " |      S.index(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found, \n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  isalnum(...)\n",
      " |      S.isalnum() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are alphanumeric\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isalpha(...)\n",
      " |      S.isalpha() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are alphabetic\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isdecimal(...)\n",
      " |      S.isdecimal() -> bool\n",
      " |      \n",
      " |      Return True if there are only decimal characters in S,\n",
      " |      False otherwise.\n",
      " |  \n",
      " |  isdigit(...)\n",
      " |      S.isdigit() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are digits\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  isidentifier(...)\n",
      " |      S.isidentifier() -> bool\n",
      " |      \n",
      " |      Return True if S is a valid identifier according\n",
      " |      to the language definition.\n",
      " |      \n",
      " |      Use keyword.iskeyword() to test for reserved identifiers\n",
      " |      such as \"def\" and \"class\".\n",
      " |  \n",
      " |  islower(...)\n",
      " |      S.islower() -> bool\n",
      " |      \n",
      " |      Return True if all cased characters in S are lowercase and there is\n",
      " |      at least one cased character in S, False otherwise.\n",
      " |  \n",
      " |  isnumeric(...)\n",
      " |      S.isnumeric() -> bool\n",
      " |      \n",
      " |      Return True if there are only numeric characters in S,\n",
      " |      False otherwise.\n",
      " |  \n",
      " |  isprintable(...)\n",
      " |      S.isprintable() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are considered\n",
      " |      printable in repr() or S is empty, False otherwise.\n",
      " |  \n",
      " |  isspace(...)\n",
      " |      S.isspace() -> bool\n",
      " |      \n",
      " |      Return True if all characters in S are whitespace\n",
      " |      and there is at least one character in S, False otherwise.\n",
      " |  \n",
      " |  istitle(...)\n",
      " |      S.istitle() -> bool\n",
      " |      \n",
      " |      Return True if S is a titlecased string and there is at least one\n",
      " |      character in S, i.e. upper- and titlecase characters may only\n",
      " |      follow uncased characters and lowercase characters only cased ones.\n",
      " |      Return False otherwise.\n",
      " |  \n",
      " |  isupper(...)\n",
      " |      S.isupper() -> bool\n",
      " |      \n",
      " |      Return True if all cased characters in S are uppercase and there is\n",
      " |      at least one cased character in S, False otherwise.\n",
      " |  \n",
      " |  join(...)\n",
      " |      S.join(iterable) -> str\n",
      " |      \n",
      " |      Return a string which is the concatenation of the strings in the\n",
      " |      iterable.  The separator between elements is S.\n",
      " |  \n",
      " |  ljust(...)\n",
      " |      S.ljust(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S left-justified in a Unicode string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  lower(...)\n",
      " |      S.lower() -> str\n",
      " |      \n",
      " |      Return a copy of the string S converted to lowercase.\n",
      " |  \n",
      " |  lstrip(...)\n",
      " |      S.lstrip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with leading whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  partition(...)\n",
      " |      S.partition(sep) -> (head, sep, tail)\n",
      " |      \n",
      " |      Search for the separator sep in S, and return the part before it,\n",
      " |      the separator itself, and the part after it.  If the separator is not\n",
      " |      found, return S and two empty strings.\n",
      " |  \n",
      " |  replace(...)\n",
      " |      S.replace(old, new[, count]) -> str\n",
      " |      \n",
      " |      Return a copy of S with all occurrences of substring\n",
      " |      old replaced by new.  If the optional argument count is\n",
      " |      given, only the first count occurrences are replaced.\n",
      " |  \n",
      " |  rfind(...)\n",
      " |      S.rfind(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  rindex(...)\n",
      " |      S.rindex(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  rjust(...)\n",
      " |      S.rjust(width[, fillchar]) -> str\n",
      " |      \n",
      " |      Return S right-justified in a string of length width. Padding is\n",
      " |      done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  rpartition(...)\n",
      " |      S.rpartition(sep) -> (head, sep, tail)\n",
      " |      \n",
      " |      Search for the separator sep in S, starting at the end of S, and return\n",
      " |      the part before it, the separator itself, and the part after it.  If the\n",
      " |      separator is not found, return two empty strings and S.\n",
      " |  \n",
      " |  rsplit(...)\n",
      " |      S.rsplit(sep=None, maxsplit=-1) -> list of strings\n",
      " |      \n",
      " |      Return a list of the words in S, using sep as the\n",
      " |      delimiter string, starting at the end of the string and\n",
      " |      working to the front.  If maxsplit is given, at most maxsplit\n",
      " |      splits are done. If sep is not specified, any whitespace string\n",
      " |      is a separator.\n",
      " |  \n",
      " |  rstrip(...)\n",
      " |      S.rstrip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with trailing whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  split(...)\n",
      " |      S.split(sep=None, maxsplit=-1) -> list of strings\n",
      " |      \n",
      " |      Return a list of the words in S, using sep as the\n",
      " |      delimiter string.  If maxsplit is given, at most maxsplit\n",
      " |      splits are done. If sep is not specified or is None, any\n",
      " |      whitespace string is a separator and empty strings are\n",
      " |      removed from the result.\n",
      " |  \n",
      " |  splitlines(...)\n",
      " |      S.splitlines([keepends]) -> list of strings\n",
      " |      \n",
      " |      Return a list of the lines in S, breaking at line boundaries.\n",
      " |      Line breaks are not included in the resulting list unless keepends\n",
      " |      is given and true.\n",
      " |  \n",
      " |  startswith(...)\n",
      " |      S.startswith(prefix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S starts with the specified prefix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      prefix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  strip(...)\n",
      " |      S.strip([chars]) -> str\n",
      " |      \n",
      " |      Return a copy of the string S with leading and trailing\n",
      " |      whitespace removed.\n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  swapcase(...)\n",
      " |      S.swapcase() -> str\n",
      " |      \n",
      " |      Return a copy of S with uppercase characters converted to lowercase\n",
      " |      and vice versa.\n",
      " |  \n",
      " |  title(...)\n",
      " |      S.title() -> str\n",
      " |      \n",
      " |      Return a titlecased version of S, i.e. words start with title case\n",
      " |      characters, all remaining cased characters have lower case.\n",
      " |  \n",
      " |  translate(...)\n",
      " |      S.translate(table) -> str\n",
      " |      \n",
      " |      Return a copy of the string S in which each character has been mapped\n",
      " |      through the given translation table. The table must implement\n",
      " |      lookup/indexing via __getitem__, for instance a dictionary or list,\n",
      " |      mapping Unicode ordinals to Unicode ordinals, strings, or None. If\n",
      " |      this operation raises LookupError, the character is left untouched.\n",
      " |      Characters mapped to None are deleted.\n",
      " |  \n",
      " |  upper(...)\n",
      " |      S.upper() -> str\n",
      " |      \n",
      " |      Return a copy of S converted to uppercase.\n",
      " |  \n",
      " |  zfill(...)\n",
      " |      S.zfill(width) -> str\n",
      " |      \n",
      " |      Pad a numeric string S with zeros on the left, to fill a field\n",
      " |      of the specified width. The string S is never truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  maketrans(x, y=None, z=None, /)\n",
      " |      Return a translation table usable for str.translate().\n",
      " |      \n",
      " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      " |      Character keys will be then converted to ordinals.\n",
      " |      If there are two arguments, they must be strings of equal length, and\n",
      " |      in the resulting dictionary, each character in x will be mapped to the\n",
      " |      character at the same position in y. If there is a third argument, it\n",
      " |      must be a string, whose characters will be mapped to None in the result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco 0 13 GPE\n",
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(u'FB is hiring a new VP of global policy')\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u'ORG'])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dear Sir/ Madam Madam ROOT Madam\n",
      "Greetings Greetings appos Madam\n",
      "the day day pobj of\n",
      "your request request pobj per\n",
      "I I nsubj am\n",
      "you you dobj assist\n",
      "the reservation\n",
      "details details pobj with\n",
      "*         Reservation # # ROOT #\n",
      "*         Guest Name Name ROOT Name\n",
      "Saroj Kumar Kumar appos Name\n",
      "*         Hotel Hotel appos Kumar\n",
      "Taj Dubai Dubai appos Hotel\n",
      "*         Dates Dates ROOT Dates\n",
      "Stay Stay pobj of\n",
      "*         Room Room ROOT Room\n",
      "Luxury Burj View Room Room appos Room\n",
      "King Bed Bed ROOT Bed\n",
      "*         Rate Rate ROOT Rate\n",
      "a Bit Longer â€“ â€“ dobj Stay\n",
      ": Breakfast Breakfast appos Inclusions\n",
      "complimentary shuttle shuttle appos Breakfast\n",
      "Dubai mall mall pobj to\n",
      "fixed timings timings appos Breakfast\n",
      "Wi-Fi Fi conj Breakfast\n",
      "4 devices devices dobj upto\n",
      "*         Cancellation Cancellation appos devices\n",
      "Reservations Reservations nsubjpass cancelled\n",
      "2PM - 1 day day pobj by\n",
      "arrival arrival pobj to\n",
      "a penalty penalty dobj avoid\n",
      "1 night charge charge pobj of\n",
      "taxes taxes conj charge\n",
      "us us dobj contact\n",
      "we we nsubj be\n",
      "further assistance assistance pobj of\n",
      "you you pobj to\n",
      "Warm Regards Regards ROOT Regards\n",
      "Diana Chettiar Chettiar appos Regards\n",
      "Taj Reservations Worldwide Team Team ROOT Team\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(u\"\"+sentence)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before []\n",
      "After [('FB', 0, 2, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(u\"FB is hiring a new Vice President of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# the model didn't recognise \"FB\" as an entity :(\n",
    "\n",
    "ORG = doc.vocab.strings[u'ORG']  # get hash value of entity label\n",
    "fb_ent = Span(doc, 0, 1, label=ORG) # create a Span for the new entity\n",
    "doc.ents = list(doc.ents) + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('After', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger <spacy.pipeline.Tagger object at 0x000001C2CCDD5630>\n",
      "parser <spacy.pipeline.DependencyParser object at 0x000001C2CCD10830>\n",
      "ner <spacy.pipeline.EntityRecognizer object at 0x000001C2CCD10780>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.make_doc(u'This is a sentence')   # create a Doc from raw text\n",
    "for name, proc in nlp.pipeline:             # iterate over components in order\n",
    "    doc = proc(doc)     \n",
    "    print(name, proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is a sentence"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*         Room: Luxury Burj View Room"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = []\n",
    "for sent in textlg.sents:\n",
    "    sentence.append(sent)\n",
    "\n",
    "sentence[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
