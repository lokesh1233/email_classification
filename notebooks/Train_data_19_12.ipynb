{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(New York,)\n"
     ]
    }
   ],
   "source": [
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_whitespace_entities, after='ner')\n",
    "doc = nlp(u'Hello\\nNew York')\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datawe/raw/Email_Classification/email_entity_cleansed.csv\")\n",
    "entityDf = df[df.apply(lambda x: x[\"text\"][x[\"start_char\"]:x[\"end_char\"]] == x[\"name\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityTrainData = []\n",
    "for text, item in  entityDf['text'].value_counts().items():\n",
    "#     print(item, text)\n",
    "    mulItems = entityDf[entityDf['text'] == text]\n",
    "    multipleEntities = []\n",
    "    for dta in mulItems.values:\n",
    "#         print(dta)\n",
    "        multipleEntities.append((dta[4], dta[1], dta[2]))\n",
    "#          entityTrainData.append((dta[5], ))\n",
    "    entityTrainData.append((text, {'entities':multipleEntities}))\n",
    "    \n",
    "TRAIN_DATA = entityTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "#     if model is not None:\n",
    "#         nlp = spacy.load(model)  # load existing spaCy model\n",
    "#         print(\"Loaded model '%s'\" % model)\n",
    "#     else:\n",
    "#         nlp = spacy.blank('en')  # create blank Language class\n",
    "#         print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "#         print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#         print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "#         print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "#         print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "#             print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#             print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 11.04036768617255}\n",
      "Losses {'ner': 8.994827101208504}\n",
      "Losses {'ner': 7.431158193883179}\n",
      "Losses {'ner': 7.665321009587325}\n",
      "Losses {'ner': 6.139390302466461}\n",
      "Losses {'ner': 5.7883339323906196}\n",
      "Losses {'ner': 5.10750174003755}\n",
      "Losses {'ner': 3.8834545573568042}\n",
      "Losses {'ner': 4.258766549192355}\n",
      "Losses {'ner': 4.235054012136147}\n",
      "Losses {'ner': 3.860838005761605}\n",
      "Losses {'ner': 3.0977877675606114}\n",
      "Losses {'ner': 3.543331276498473}\n",
      "Losses {'ner': 2.723645766153609}\n",
      "Losses {'ner': 2.381960735122874}\n",
      "Losses {'ner': 2.202290919946724}\n",
      "Losses {'ner': 2.7878237475888192}\n",
      "Losses {'ner': 1.801100408717077}\n",
      "Losses {'ner': 1.636139536098428}\n",
      "Losses {'ner': 1.877713051378941}\n",
      "Losses {'ner': 1.595699860457334}\n",
      "Losses {'ner': 1.2172807294578367}\n",
      "Losses {'ner': 0.9811632195135973}\n",
      "Losses {'ner': 1.4473658214348963}\n",
      "Losses {'ner': 1.6787941906053279}\n",
      "Losses {'ner': 1.294053436717511}\n",
      "Losses {'ner': 0.5250647165754453}\n",
      "Losses {'ner': 0.6002084558009897}\n",
      "Losses {'ner': 0.7634135523698271}\n",
      "Losses {'ner': 0.5783444117414812}\n",
      "Losses {'ner': 0.355398461418797}\n",
      "Losses {'ner': 0.7211753642569031}\n",
      "Losses {'ner': 0.547808029710364}\n",
      "Losses {'ner': 0.6966179616300391}\n",
      "Losses {'ner': 0.9586641114642686}\n",
      "Losses {'ner': 0.4459067004028616}\n",
      "Losses {'ner': 0.7628227914950992}\n",
      "Losses {'ner': 0.6560142827130135}\n",
      "Losses {'ner': 0.21113991505364418}\n",
      "Losses {'ner': 0.5222074051780821}\n",
      "Losses {'ner': 0.454593850212992}\n",
      "Losses {'ner': 0.21107241478832686}\n",
      "Losses {'ner': 0.3765818180678788}\n",
      "Losses {'ner': 0.014507726462716721}\n",
      "Losses {'ner': 0.4533811608221041}\n",
      "Losses {'ner': 0.01404645072548446}\n",
      "Losses {'ner': 0.19040723407501628}\n",
      "Losses {'ner': 0.756868837349418}\n",
      "Losses {'ner': 0.2751277759246275}\n",
      "Losses {'ner': 0.3560319773384818}\n",
      "Losses {'ner': 0.42769410788417483}\n",
      "Losses {'ner': 0.26639286387708855}\n",
      "Losses {'ner': 0.42222767051454174}\n",
      "Losses {'ner': 0.14019701384821906}\n",
      "Losses {'ner': 0.19432915187561475}\n",
      "Losses {'ner': 0.11684440700173886}\n",
      "Losses {'ner': 0.19440964157832885}\n",
      "Losses {'ner': 0.3380314324325868}\n",
      "Losses {'ner': 0.03914011135683788}\n",
      "Losses {'ner': 0.6248726403379852}\n",
      "Losses {'ner': 0.05689121402918809}\n",
      "Losses {'ner': 0.33215311231188377}\n",
      "Losses {'ner': 0.20034238353775727}\n",
      "Losses {'ner': 0.17651825266703078}\n",
      "Losses {'ner': 0.2595368069617439}\n",
      "Losses {'ner': 0.06505521179385541}\n",
      "Losses {'ner': 0.13362457658587198}\n",
      "Losses {'ner': 0.2618922003016188}\n",
      "Losses {'ner': 0.3839514369131919}\n",
      "Losses {'ner': 0.39659542402741904}\n",
      "Losses {'ner': 0.07447099076081172}\n",
      "Losses {'ner': 0.21215552647221292}\n",
      "Losses {'ner': 0.0739473554309069}\n",
      "Losses {'ner': 0.007779393362324738}\n",
      "Losses {'ner': 0.5450674719522584}\n",
      "Losses {'ner': 0.00901659292568297}\n",
      "Losses {'ner': 0.06157198617312633}\n",
      "Losses {'ner': 0.0013250089944602408}\n",
      "Losses {'ner': 0.12467255137817537}\n",
      "Losses {'ner': 0.003024034764972941}\n",
      "Losses {'ner': 0.09923560116970827}\n",
      "Losses {'ner': 0.029565045530806883}\n",
      "Losses {'ner': 0.12158969675240418}\n",
      "Losses {'ner': 0.3324671826728397}\n",
      "Losses {'ner': 0.0006895800352353068}\n",
      "Losses {'ner': 0.2233773866067767}\n",
      "Losses {'ner': 0.20224155827681298}\n",
      "Losses {'ner': 0.02036563360595582}\n",
      "Losses {'ner': 0.0348064362784028}\n",
      "Losses {'ner': 7.099635335266723e-05}\n",
      "Losses {'ner': 0.07120286225973646}\n",
      "Losses {'ner': 0.025795421491979775}\n",
      "Losses {'ner': 0.22479220877824374}\n",
      "Losses {'ner': 0.03339113624729806}\n",
      "Losses {'ner': 0.18588564392061793}\n",
      "Losses {'ner': 0.23090797459172013}\n",
      "Losses {'ner': 0.02874157203750521}\n",
      "Losses {'ner': 0.05423751540492389}\n",
      "Losses {'ner': 0.048184531256570316}\n",
      "Losses {'ner': 0.1079624670126751}\n"
     ]
    }
   ],
   "source": [
    "trainedModel = main_train('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lake palace 22 33 FAC\n",
      "1st January 2019 85 101 DATE\n",
      "lake palace 124 135 FAC\n",
      "1st 187 190 DATE\n",
      "lake palace 226 237 FAC\n",
      "1st 289 292 DATE\n",
      "lake palace 328 339 FAC\n",
      "1st January 2019 391 407 DATE\n",
      "lake palace 430 441 FAC\n",
      "1st January 2019 493 509 DATE\n",
      "lake palace 532 543 FAC\n",
      "1st January 2019 595 611 DATE\n",
      "Taj Gateway Ganges 710 728 ORG\n",
      "24/26 December 2018 734 753 DATE\n",
      "Taj Gateway Ganges 870 888 ORG\n",
      "24/26 December 2018 894 913 DATE\n",
      "17/12/2018 1066 1076 DATE\n",
      "TEHP/18/12809 1580 1593 CARDINAL\n",
      "Arun Gadamshetty P 1601 1619 PERSON\n",
      "153021 1626 1632 CARDINAL\n",
      "Spares Manufacturing Department 1646 1677 ORG\n",
      "Vivanta 1710 1717 GPE\n",
      "Taj - Madikeri Coorg 1721 1741 ORG\n",
      "02:00 PM 1809 1817 TIME\n",
      "12:00 PM 1852 1860 TIME\n",
      "1 1875 1876 CARDINAL\n",
      "Arun Gadamshetty   1952 1970 PERSON\n",
      "34 years 2 1993 2003 DATE\n",
      "SHWETA GADAMSHETTY 2013 2031 PERSON\n",
      "28 years 2058 2066 DATE\n",
      "TEHP/18/12809 2556 2569 CARDINAL\n",
      "Arun Gadamshetty P 2577 2595 PERSON\n",
      "153021 2602 2608 CARDINAL\n",
      "Spares Manufacturing Department 2622 2653 ORG\n",
      "Vivanta 2686 2693 GPE\n",
      "Taj - Madikeri Coorg 2697 2717 ORG\n",
      "02:00 PM 2785 2793 TIME\n",
      "12:00 PM 2828 2836 TIME\n",
      "1 2851 2852 CARDINAL\n",
      "Arun Gadamshetty   2928 2946 PERSON\n",
      "34 years 2 2969 2979 DATE\n",
      "SHWETA GADAMSHETTY 2989 3007 PERSON\n",
      "28 years 3034 3042 DATE\n",
      "TEHP/18/12809 3532 3545 CARDINAL\n",
      "Arun Gadamshetty P 3553 3571 PERSON\n",
      "153021 3578 3584 CARDINAL\n",
      "Spares Manufacturing Department 3598 3629 ORG\n",
      "Vivanta 3662 3669 GPE\n",
      "Taj - Madikeri Coorg 3673 3693 ORG\n",
      "02:00 PM 3761 3769 TIME\n",
      "12:00 PM 3804 3812 TIME\n",
      "1 3827 3828 CARDINAL\n",
      "Arun Gadamshetty   3904 3922 PERSON\n",
      "34 years 2 3945 3955 DATE\n",
      "SHWETA GADAMSHETTY 3965 3983 PERSON\n",
      "28 years 4010 4018 DATE\n",
      "TEHP/18/12809 4508 4521 CARDINAL\n",
      "Arun Gadamshetty P 4529 4547 PERSON\n",
      "153021 4554 4560 CARDINAL\n",
      "Spares Manufacturing Department 4574 4605 ORG\n",
      "Vivanta 4638 4645 GPE\n",
      "Taj - Madikeri Coorg 4649 4669 ORG\n",
      "02:00 PM 4737 4745 TIME\n",
      "12:00 PM 4780 4788 TIME\n",
      "1 4803 4804 CARDINAL\n",
      "Arun Gadamshetty   4880 4898 PERSON\n",
      "34 years 2 4921 4931 DATE\n",
      "SHWETA GADAMSHETTY 4941 4959 PERSON\n",
      "28 years 4986 4994 DATE\n",
      "TEHP/18/12809 5484 5497 CARDINAL\n",
      "Arun Gadamshetty P 5505 5523 PERSON\n",
      "153021 5530 5536 CARDINAL\n",
      "Spares Manufacturing Department 5550 5581 ORG\n",
      "Vivanta 5614 5621 GPE\n",
      "Taj - Madikeri Coorg 5625 5645 ORG\n",
      "02:00 PM 5713 5721 TIME\n",
      "12:00 PM 5756 5764 TIME\n",
      "1 5779 5780 CARDINAL\n",
      "Arun Gadamshetty   5856 5874 PERSON\n",
      "34 years 2 5897 5907 DATE\n",
      "SHWETA GADAMSHETTY 5917 5935 PERSON\n",
      "28 years 5962 5970 DATE\n",
      "TEHP/18/12809 6460 6473 CARDINAL\n",
      "Arun Gadamshetty P 6481 6499 PERSON\n",
      "153021 6506 6512 CARDINAL\n",
      "Spares Manufacturing Department 6526 6557 ORG\n",
      "Vivanta 6590 6597 GPE\n",
      "Taj - Madikeri Coorg 6601 6621 ORG\n",
      "02:00 PM 6689 6697 TIME\n",
      "12:00 PM 6732 6740 TIME\n",
      "1 6755 6756 CARDINAL\n",
      "Arun Gadamshetty   6832 6850 PERSON\n",
      "34 years 2 6873 6883 DATE\n",
      "SHWETA GADAMSHETTY 6893 6911 PERSON\n",
      "28 years 6938 6946 DATE\n",
      "TEHP/18/12809 7436 7449 CARDINAL\n",
      "Arun Gadamshetty P 7457 7475 PERSON\n",
      "153021 7482 7488 CARDINAL\n",
      "Spares Manufacturing Department 7502 7533 ORG\n",
      "Vivanta 7566 7573 GPE\n",
      "Taj - Madikeri Coorg 7577 7597 ORG\n",
      "02:00 PM 7665 7673 TIME\n",
      "12:00 PM 7708 7716 TIME\n",
      "1 7731 7732 CARDINAL\n",
      "Arun Gadamshetty   7808 7826 PERSON\n",
      "34 years 2 7849 7859 DATE\n",
      "SHWETA GADAMSHETTY 7869 7887 PERSON\n",
      "28 years 7914 7922 DATE\n",
      "TEHP/18/12809 8412 8425 CARDINAL\n",
      "Arun Gadamshetty P 8433 8451 PERSON\n",
      "153021 8458 8464 CARDINAL\n",
      "Spares Manufacturing Department 8478 8509 ORG\n",
      "Vivanta 8542 8549 GPE\n",
      "Taj - Madikeri Coorg 8553 8573 ORG\n",
      "02:00 PM 8641 8649 TIME\n",
      "12:00 PM 8684 8692 TIME\n",
      "1 8707 8708 CARDINAL\n",
      "Arun Gadamshetty   8784 8802 PERSON\n",
      "34 years 2 8825 8835 DATE\n",
      "SHWETA GADAMSHETTY 8845 8863 PERSON\n",
      "28 years 8890 8898 DATE\n",
      "TEHP/18/12809 9388 9401 CARDINAL\n",
      "Arun Gadamshetty P 9409 9427 PERSON\n",
      "153021 9434 9440 CARDINAL\n",
      "Spares Manufacturing Department 9454 9485 ORG\n",
      "Vivanta 9518 9525 GPE\n",
      "Taj - Madikeri Coorg 9529 9549 ORG\n",
      "02:00 PM 9617 9625 TIME\n",
      "12:00 PM 9660 9668 TIME\n",
      "1 9683 9684 CARDINAL\n",
      "Arun Gadamshetty   9760 9778 PERSON\n",
      "34 years 2 9801 9811 DATE\n",
      "SHWETA GADAMSHETTY 9821 9839 PERSON\n",
      "28 years 9866 9874 DATE\n",
      "TEHP/18/12809 10364 10377 CARDINAL\n",
      "Arun Gadamshetty P 10385 10403 PERSON\n",
      "153021 10410 10416 CARDINAL\n",
      "Spares Manufacturing Department 10430 10461 ORG\n",
      "Vivanta 10494 10501 GPE\n",
      "Taj - Madikeri Coorg 10505 10525 ORG\n",
      "02:00 PM 10593 10601 TIME\n",
      "12:00 PM 10636 10644 TIME\n",
      "1 10659 10660 CARDINAL\n",
      "Arun Gadamshetty   10736 10754 PERSON\n",
      "34 years 2 10777 10787 DATE\n",
      "SHWETA GADAMSHETTY 10797 10815 PERSON\n",
      "28 years 10842 10850 DATE\n",
      "TEHP/18/12809 11340 11353 CARDINAL\n",
      "Arun Gadamshetty P 11361 11379 PERSON\n",
      "153021 11386 11392 CARDINAL\n",
      "Spares Manufacturing Department 11406 11437 ORG\n",
      "Vivanta 11470 11477 GPE\n",
      "Taj - Madikeri Coorg 11481 11501 ORG\n",
      "02:00 PM 11569 11577 TIME\n",
      "12:00 PM 11612 11620 TIME\n",
      "1 11635 11636 CARDINAL\n",
      "Arun Gadamshetty   11712 11730 PERSON\n",
      "34 years 2 11753 11763 DATE\n",
      "SHWETA GADAMSHETTY 11773 11791 PERSON\n",
      "28 years 11818 11826 DATE\n",
      "TEHP/18/12809 12316 12329 CARDINAL\n",
      "Arun Gadamshetty P 12337 12355 PERSON\n",
      "153021 12362 12368 CARDINAL\n",
      "Spares Manufacturing Department 12382 12413 ORG\n",
      "Vivanta 12446 12453 GPE\n",
      "Taj - Madikeri Coorg 12457 12477 ORG\n",
      "02:00 PM 12545 12553 TIME\n",
      "12:00 PM 12588 12596 TIME\n",
      "1 12611 12612 CARDINAL\n",
      "Arun Gadamshetty   12688 12706 PERSON\n",
      "34 years 2 12729 12739 DATE\n",
      "SHWETA GADAMSHETTY 12749 12767 PERSON\n",
      "28 years 12794 12802 DATE\n",
      "TEHP/18/12809 13292 13305 CARDINAL\n",
      "Arun Gadamshetty P 13313 13331 PERSON\n",
      "153021 13338 13344 CARDINAL\n",
      "Spares Manufacturing Department 13358 13389 ORG\n",
      "Vivanta 13422 13429 GPE\n",
      "Taj - Madikeri Coorg 13433 13453 ORG\n",
      "02:00 PM 13521 13529 TIME\n",
      "12:00 PM 13564 13572 TIME\n",
      "1 13587 13588 CARDINAL\n",
      "Arun Gadamshetty   13664 13682 PERSON\n",
      "34 years 2 13705 13715 DATE\n",
      "SHWETA GADAMSHETTY 13725 13743 PERSON\n",
      "28 years 13770 13778 DATE\n",
      "Taj Mahal 14239 14248 FAC\n",
      "New Delhi 14250 14259 GPE\n",
      "24/26 Dec 2018 14265 14279 DATE\n",
      "Taj Mahal 14435 14444 FAC\n",
      "New Delhi 14446 14455 GPE\n",
      "24/26 Dec 2018 14461 14475 DATE\n",
      "Taj Mahal 14631 14640 FAC\n",
      "New Delhi 14642 14651 GPE\n",
      "24/26 Dec 2018 14657 14671 DATE\n",
      "9th DECEMBER 2018 14910 14927 DATE\n",
      "11th DECEMBER 2018 14943 14961 DATE\n",
      "9th DECEMBER 2018 15200 15217 DATE\n",
      "11th DECEMBER 2018 15233 15251 DATE\n",
      "Star 15366 15370 ORG\n",
      "Lake Palace 15405 15416 FAC\n",
      "31st 15437 15441 DATE\n",
      "1st January 2019 15468 15484 DATE\n",
      "Lake Palace 15510 15521 FAC\n",
      "31st 15542 15546 DATE\n",
      "1st January 2019 15573 15589 DATE\n",
      "Lake Palace 15615 15626 FAC\n",
      "31st 15647 15651 DATE\n",
      "1st January 2019 15678 15694 DATE\n",
      "the Taj Mahal 15737 15750 LOC\n",
      "Mumbai 15754 15760 GPE\n",
      "Mumbai 15813 15819 GPE\n",
      "101010203753 15968 15980 DATE\n",
      "the Taj Mahal 16038 16051 LOC\n",
      "Mumbai 16055 16061 GPE\n",
      "Mumbai 16114 16120 GPE\n",
      "101010203753 16269 16281 DATE\n",
      "the Taj Mahal 16339 16352 LOC\n",
      "Mumbai 16356 16362 GPE\n",
      "Mumbai 16415 16421 GPE\n",
      "101010203753 16570 16582 DATE\n",
      "the Taj Mahal 16640 16653 LOC\n",
      "Mumbai 16657 16663 GPE\n",
      "Mumbai 16716 16722 GPE\n",
      "101010203753 16871 16883 DATE\n",
      "ABB 16953 16956 ORG\n",
      "Bangalore 16967 16976 GPE\n",
      "Monday 26 November to Thursday 29 November 17016 17058 DATE\n",
      "1 17064 17065 CARDINAL\n",
      "Airport Limousine 17092 17109 ORG\n",
      "Bengaluru Airport 17124 17141 FAC\n",
      "61 428 856 843 17163 17177 CARDINAL\n",
      "ABB 17300 17303 ORG\n",
      "Bangalore 17314 17323 GPE\n",
      "Monday 26 November to Thursday 29 November 17363 17405 DATE\n",
      "1 17411 17412 CARDINAL\n",
      "Airport Limousine 17439 17456 ORG\n",
      "Bengaluru Airport 17471 17488 FAC\n",
      "61 428 856 843 17510 17524 CARDINAL\n",
      "ABB 17647 17650 ORG\n",
      "Bangalore 17661 17670 GPE\n",
      "Monday 26 November to Thursday 29 November 17710 17752 DATE\n",
      "1 17758 17759 CARDINAL\n",
      "Airport Limousine 17786 17803 ORG\n",
      "Bengaluru Airport 17818 17835 FAC\n",
      "61 428 856 843 17857 17871 CARDINAL\n",
      "ABB 17994 17997 ORG\n",
      "Bangalore 18008 18017 GPE\n",
      "Monday 26 November to Thursday 29 November 18057 18099 DATE\n",
      "1 18105 18106 CARDINAL\n",
      "Airport Limousine 18133 18150 ORG\n",
      "Bengaluru Airport 18165 18182 FAC\n",
      "61 428 856 843 18204 18218 CARDINAL\n",
      "ABB 18341 18344 ORG\n",
      "Bangalore 18355 18364 GPE\n",
      "Monday 26 November to Thursday 29 November 18404 18446 DATE\n",
      "1 18452 18453 CARDINAL\n",
      "Airport Limousine 18480 18497 ORG\n",
      "Bengaluru Airport 18512 18529 FAC\n",
      "61 428 856 843 18551 18565 CARDINAL\n",
      "ABB 18688 18691 ORG\n",
      "Bangalore 18702 18711 GPE\n",
      "Monday 26 November to Thursday 29 November 18751 18793 DATE\n",
      "1 18799 18800 CARDINAL\n",
      "Airport Limousine 18827 18844 ORG\n",
      "Bengaluru Airport 18859 18876 FAC\n",
      "61 428 856 843 18898 18912 CARDINAL\n",
      "ABB 19035 19038 ORG\n",
      "Bangalore 19049 19058 GPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday 26 November to Thursday 29 November 19098 19140 DATE\n",
      "1 19146 19147 CARDINAL\n",
      "Airport Limousine 19174 19191 ORG\n",
      "Bengaluru Airport 19206 19223 FAC\n",
      "61 428 856 843 19245 19259 CARDINAL\n",
      "Dubai 19437 19442 GPE\n",
      "1 19447 19448 CARDINAL\n",
      "28 November – 01 December 2018 19459 19489 DATE\n",
      "3 19495 19496 QUANTITY\n",
      "Dubai 19616 19621 GPE\n",
      "1 19626 19627 CARDINAL\n",
      "28 November – 01 December 2018 19638 19668 DATE\n",
      "3 19674 19675 QUANTITY\n",
      "Dubai 19795 19800 GPE\n",
      "1 19805 19806 CARDINAL\n",
      "28 November – 01 December 2018 19817 19847 DATE\n",
      "3 19853 19854 QUANTITY\n",
      "Dubai 19974 19979 GPE\n",
      "1 19984 19985 CARDINAL\n",
      "28 November – 01 December 2018 19996 20026 DATE\n",
      "3 20032 20033 QUANTITY\n",
      "4 20093 20094 CARDINAL\n",
      "12th Jan, 2019 20100 20114 DATE\n",
      "15th Jan, 2019 20118 20132 ORDINAL\n",
      "2 20134 20135 CARDINAL\n",
      "2 20147 20148 CARDINAL\n",
      "16 20155 20157 QUANTITY\n",
      "4 20309 20310 CARDINAL\n",
      "12th Jan, 2019 20316 20330 DATE\n",
      "15th Jan, 2019 20334 20348 ORDINAL\n",
      "2 20350 20351 CARDINAL\n",
      "2 20363 20364 CARDINAL\n",
      "16 20371 20373 QUANTITY\n",
      "4 20525 20526 CARDINAL\n",
      "12th Jan, 2019 20532 20546 DATE\n",
      "15th Jan, 2019 20550 20564 ORDINAL\n",
      "2 20566 20567 CARDINAL\n",
      "2 20579 20580 CARDINAL\n",
      "16 20587 20589 QUANTITY\n",
      "4 20741 20742 CARDINAL\n",
      "12th Jan, 2019 20748 20762 DATE\n",
      "15th Jan, 2019 20766 20780 ORDINAL\n",
      "2 20782 20783 CARDINAL\n",
      "2 20795 20796 CARDINAL\n",
      "16 20803 20805 QUANTITY\n",
      "4 20957 20958 CARDINAL\n",
      "12th Jan, 2019 20964 20978 DATE\n",
      "15th Jan, 2019 20982 20996 ORDINAL\n",
      "2 20998 20999 CARDINAL\n",
      "2 21011 21012 CARDINAL\n",
      "16 21019 21021 QUANTITY\n",
      "4 21173 21174 CARDINAL\n",
      "12th Jan, 2019 21180 21194 DATE\n",
      "15th Jan, 2019 21198 21212 ORDINAL\n",
      "2 21214 21215 CARDINAL\n",
      "2 21227 21228 CARDINAL\n",
      "16 21235 21237 QUANTITY\n",
      "Charunya 21348 21356 PERSON\n",
      "4 21441 21442 CARDINAL\n",
      "two 21511 21514 CARDINAL\n",
      "Charunya 21526 21534 PERSON\n",
      "4 21619 21620 CARDINAL\n",
      "two 21689 21692 CARDINAL\n",
      "Charunya 21704 21712 PERSON\n",
      "4 21797 21798 CARDINAL\n",
      "two 21867 21870 CARDINAL\n",
      "3 21939 21940 CARDINAL\n",
      "2.30am 22120 22126 TIME\n",
      "2.30am 22151 22157 TIME\n",
      "3 22305 22306 CARDINAL\n",
      "2.30am 22486 22492 TIME\n",
      "2.30am 22517 22523 TIME\n",
      "3 22671 22672 CARDINAL\n",
      "2.30am 22852 22858 TIME\n",
      "2.30am 22883 22889 TIME\n",
      "Dubai 23017 23022 GPE\n",
      "Gaurav 23033 23039 PERSON\n",
      "2 nights 23076 23084 DATE\n",
      "Taj Agauda 23088 23098 PERSON\n",
      "17th April 2019 23108 23123 DATE\n",
      "19th April 2019 23127 23142 DATE\n",
      "Gaurav 23156 23162 PERSON\n",
      "2 nights 23199 23207 DATE\n",
      "Taj Agauda 23211 23221 PERSON\n",
      "17th April 2019 23231 23246 DATE\n",
      "19th April 2019 23250 23265 DATE\n",
      "Gaurav 23279 23285 PERSON\n",
      "2 nights 23322 23330 DATE\n",
      "Taj Agauda 23334 23344 PERSON\n",
      "17th April 2019 23354 23369 DATE\n",
      "19th April 2019 23373 23388 DATE\n",
      "Gaurav 23402 23408 PERSON\n",
      "2 nights 23445 23453 DATE\n",
      "Taj Agauda 23457 23467 PERSON\n",
      "17th April 2019 23477 23492 DATE\n",
      "19th April 2019 23496 23511 DATE\n",
      "Gaurav 23525 23531 PERSON\n",
      "2 nights 23568 23576 DATE\n",
      "Taj Agauda 23580 23590 PERSON\n",
      "17th April 2019 23600 23615 DATE\n",
      "19th April 2019 23619 23634 DATE\n",
      "Mahesh 23648 23654 PERSON\n",
      "Taj Cape Town 23706 23719 FAC\n",
      "Mahesh 23729 23735 PERSON\n",
      "Taj Cape Town 23787 23800 FAC\n",
      "Shevawn Barder 23971 23985 PERSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = trainedModel(u''+ \" \".join(entityDf[\"text\"]))\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake palace 25 36 FAC\n",
      "20th December 2018 56 74 DATE\n",
      "1st January 2019 87 103 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = trainedModel(u''+ \"\"\"i want to book a room at Lake palace Check in date is - 20th December 2018 check out - 1st January 2019\"\"\")\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>end_char</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>start_char</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>83</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Star</td>\n",
       "      <td>79</td>\n",
       "      <td>Dear Team,   Please cancel the attached bookin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  end_char label  name  start_char  \\\n",
       "27          27        83   ORG  Star          79   \n",
       "\n",
       "                                                 text  \n",
       "27  Dear Team,   Please cancel the attached bookin...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"i want to book a room at Lake palace \n",
    "Check in date is - 31st December 2018 \n",
    "check out - 1st January 2019\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake palace 25 36 FAC\n",
      "31st December 2018 57 75 DATE\n",
      "1st January 2019 89 105 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = trainedModel(u''+ sentence.replace('\\n', ' '))\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-6a7424a3435f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Creating the term dictionary of our corpus, where every unique term is assigned an index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mINFO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corpus'"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corpus in c:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-2f800d08ede1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mINFO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corpus'"
     ]
    }
   ],
   "source": [
    "import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
