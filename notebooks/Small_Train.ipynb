{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### html browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, render_template \n",
    "# app=Flask(__name__) \n",
    "# @app.route(\"/\") \n",
    "# def home(): \n",
    "#     return render_template(\"../src/home.html\") \n",
    "# if __name__ ==\"__main__\": \n",
    "#     app.run(debug=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(New York,)\n"
     ]
    }
   ],
   "source": [
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_whitespace_entities, after='ner')\n",
    "doc = nlp(u'Hello\\nNew York')\n",
    "print(doc.ents)\n",
    "# (New York,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datawe/raw/Email_Classification/small_train.csv\")\n",
    "entityDf = df[df.apply(lambda x: x[\"text\"][x[\"start_char\"]:x[\"end_char\"]] == x[\"Entity\"], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityTrainData = []\n",
    "for text, item in  entityDf['text'].value_counts().items():\n",
    "#     print(item, text)\n",
    "    mulItems = entityDf[entityDf['text'] == text]\n",
    "    multipleEntities = []\n",
    "    for dta in mulItems.values:\n",
    "#         print(dta)\n",
    "        multipleEntities.append((dta[4], dta[2], dta[3]))\n",
    "#          entityTrainData.append((dta[5], ))\n",
    "    entityTrainData.append((text, {'entities':multipleEntities}))\n",
    "    \n",
    "TRAIN_DATA = entityTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entityDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(model=None, output_dir=None, n_iter=120):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "#         print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#         print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "#         print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "#         print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "#             print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#             print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Losses {'ner': 3.315060920795562}\n",
      "Losses {'ner': 3.404045535647962}\n",
      "Losses {'ner': 3.930275830262143}\n",
      "Losses {'ner': 3.06657715512887}\n",
      "Losses {'ner': 2.7897050859087575}\n",
      "Losses {'ner': 3.4477013516382}\n",
      "Losses {'ner': 2.856652928588005}\n",
      "Losses {'ner': 2.198723471604673}\n",
      "Losses {'ner': 3.9322881182655935}\n",
      "Losses {'ner': 2.0247766211214935}\n",
      "Losses {'ner': 0.9683140641037671}\n",
      "Losses {'ner': 0.9164910951901817}\n",
      "Losses {'ner': 2.3304153550250812}\n",
      "Losses {'ner': 1.0981210743502539}\n",
      "Losses {'ner': 1.1953628250814463}\n",
      "Losses {'ner': 0.8796425276903744}\n",
      "Losses {'ner': 0.9564163050529851}\n",
      "Losses {'ner': 1.242199325988068}\n",
      "Losses {'ner': 1.2970445863195614}\n",
      "Losses {'ner': 1.3454837944351492}\n",
      "Losses {'ner': 1.5739944373588446}\n",
      "Losses {'ner': 1.2971613946319005}\n",
      "Losses {'ner': 0.5468275043892331}\n",
      "Losses {'ner': 0.5687733131732137}\n",
      "Losses {'ner': 0.29721300995993744}\n",
      "Losses {'ner': 0.5135165650669608}\n",
      "Losses {'ner': 0.548890644508621}\n",
      "Losses {'ner': 0.3721958532234902}\n",
      "Losses {'ner': 0.6185573880332338}\n",
      "Losses {'ner': 0.24064163195032418}\n",
      "Losses {'ner': 0.580365214687619}\n",
      "Losses {'ner': 0.35032133215616956}\n",
      "Losses {'ner': 0.6524083028984485}\n",
      "Losses {'ner': 0.18188498322070482}\n",
      "Losses {'ner': 0.9900114167585823}\n",
      "Losses {'ner': 0.515087231131008}\n",
      "Losses {'ner': 0.14208686661545952}\n",
      "Losses {'ner': 0.1569600693851106}\n",
      "Losses {'ner': 0.5550153964558578}\n",
      "Losses {'ner': 0.9178285044206017}\n",
      "Losses {'ner': 0.256240354939596}\n",
      "Losses {'ner': 0.21770353053845937}\n",
      "Losses {'ner': 0.5168804666537399}\n",
      "Losses {'ner': 0.005458921012253817}\n",
      "Losses {'ner': 0.5992065570115412}\n",
      "Losses {'ner': 0.7714683953993924}\n",
      "Losses {'ner': 0.30144714704132825}\n",
      "Losses {'ner': 0.23967207128249682}\n",
      "Losses {'ner': 0.39999506764805864}\n",
      "Losses {'ner': 0.6748819964764012}\n",
      "Losses {'ner': 0.3655447171999484}\n",
      "Losses {'ner': 0.6336787999609368}\n",
      "Losses {'ner': 0.5879055594828442}\n",
      "Losses {'ner': 0.1869378157611395}\n",
      "Losses {'ner': 0.0004430746853574363}\n",
      "Losses {'ner': 0.6890070178110327}\n",
      "Losses {'ner': 0.3229587756609518}\n",
      "Losses {'ner': 0.6821337551317711}\n",
      "Losses {'ner': 0.5527252782805371}\n",
      "Losses {'ner': 0.7421609291446303}\n",
      "Losses {'ner': 0.36963941550758117}\n",
      "Losses {'ner': 0.0005422154946625457}\n",
      "Losses {'ner': 0.009985967517522612}\n",
      "Losses {'ner': 0.5322486071639587}\n",
      "Losses {'ner': 0.21725180150715545}\n",
      "Losses {'ner': 0.1733711960339692}\n",
      "Losses {'ner': 0.24595729306419906}\n",
      "Losses {'ner': 0.12716379165826136}\n",
      "Losses {'ner': 0.5584080726087085}\n",
      "Losses {'ner': 0.4571926962594318}\n",
      "Losses {'ner': 0.006239456199060417}\n",
      "Losses {'ner': 0.12392047650689114}\n",
      "Losses {'ner': 0.6502526427707315}\n",
      "Losses {'ner': 0.1333665339196517}\n",
      "Losses {'ner': 0.33211010483339914}\n",
      "Losses {'ner': 0.5007760315503054}\n",
      "Losses {'ner': 0.1312518733678294}\n",
      "Losses {'ner': 0.21703554588934898}\n",
      "Losses {'ner': 0.26251445694178704}\n",
      "Losses {'ner': 0.9556318687872519}\n",
      "Losses {'ner': 0.1884447526501492}\n",
      "Losses {'ner': 0.5100070939914292}\n",
      "Losses {'ner': 0.00800053694675632}\n",
      "Losses {'ner': 0.47895569222298207}\n",
      "Losses {'ner': 0.034601886920540136}\n",
      "Losses {'ner': 0.0834050706329293}\n",
      "Losses {'ner': 0.1941081965143661}\n",
      "Losses {'ner': 1.1157539890998973}\n",
      "Losses {'ner': 0.08313644136822318}\n",
      "Losses {'ner': 0.005138392615675735}\n",
      "Losses {'ner': 0.6397618298437058}\n",
      "Losses {'ner': 0.4066506287329931}\n",
      "Losses {'ner': 0.12540133945616028}\n",
      "Losses {'ner': 0.09106877531137471}\n",
      "Losses {'ner': 0.14175825823316357}\n",
      "Losses {'ner': 0.4450032674252111}\n",
      "Losses {'ner': 0.004785474552220111}\n",
      "Losses {'ner': 0.05839180776444121}\n",
      "Losses {'ner': 0.12415920881684943}\n",
      "Losses {'ner': 0.12351548324928187}\n",
      "Losses {'ner': 0.34579009211379963}\n",
      "Losses {'ner': 0.11235447164420445}\n",
      "Losses {'ner': 1.182203838034339}\n",
      "Losses {'ner': 0.5153444076756758}\n",
      "Losses {'ner': 0.28128035571869525}\n",
      "Losses {'ner': 0.5332173223545555}\n",
      "Losses {'ner': 0.4103683162165829}\n",
      "Losses {'ner': 0.9764453654587696}\n",
      "Losses {'ner': 0.08268548333676783}\n",
      "Losses {'ner': 0.15117340428774032}\n",
      "Losses {'ner': 0.12773428929905647}\n",
      "Losses {'ner': 0.44474111215962264}\n",
      "Losses {'ner': 1.0596167690388703}\n",
      "Losses {'ner': 0.03901841149867516}\n",
      "Losses {'ner': 0.19287197902087116}\n",
      "Losses {'ner': 0.10224705612627827}\n",
      "Losses {'ner': 0.45968994982829614}\n",
      "Losses {'ner': 0.06485309624779882}\n",
      "Losses {'ner': 0.32359440453689925}\n",
      "Losses {'ner': 0.7472928199135936}\n"
     ]
    }
   ],
   "source": [
    "trainedModel = main_train('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('check in and check out date 31/12/2018, 1/1/2019',\n",
       "  {'entities': [(28, 38, 'DATE'), (40, 48, 'DATE')]}),\n",
       " ('i want to book a room at lake palace. Check in date is - 31st December 2018, check out - 1st January 2019',\n",
       "  {'entities': [(25, 36, 'LOC'), (57, 75, 'DATE')]}),\n",
       " ('Dear Rohit,   As per our conversation a while ago, please book and confirm 02 Executive room at Taj Gateway Ganges from 24/26 December 2018 for two nights.',\n",
       "  {'entities': [(5, 10, 'PERSON'),\n",
       "    (96, 114, 'ORG'),\n",
       "    (120, 139, 'DATE'),\n",
       "    (144, 154, 'DATE')]}),\n",
       " ('i want to book a room at lake palace. Check in date is - 20th december 2018, check out - 8th January 2019',\n",
       "  {'entities': [(89, 105, 'DATE')]}),\n",
       " ('As per corporate holiday plan, kindly arrange to book the following accommodation in your Hotel/Resort as per details given below: Serial number : TEHP/18/12809 Name : Arun Gadamshetty P No. : 153021 Department : Spares Manufacturing Department ',\n",
       "  {'entities': [(90, 102, 'FAC'),\n",
       "    (147, 160, 'ORG'),\n",
       "    (168, 184, 'PERSON'),\n",
       "    (193, 199, 'CARDINAL'),\n",
       "    (200, 244, 'ORG')]}),\n",
       " ('Hi Agnes,   I have not received a response to this email as yet.',\n",
       "  {'entities': [(3, 8, 'PERSON')]})]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rohit 5 10 PERSON\n",
      "Taj Gateway Ganges 96 114 ORG\n",
      "24/26 December 2018 120 139 DATE\n",
      "two nights 144 154 DATE\n",
      "Rohit 161 166 PERSON\n",
      "Taj Gateway Ganges 252 270 ORG\n",
      "24/26 December 2018 276 295 DATE\n",
      "two nights 300 310 DATE\n",
      "Rohit 317 322 PERSON\n",
      "Taj Gateway Ganges 408 426 ORG\n",
      "24/26 December 2018 432 451 DATE\n",
      "two nights 456 466 DATE\n",
      "Rohit 473 478 PERSON\n",
      "Taj Gateway Ganges 564 582 ORG\n",
      "24/26 December 2018 588 607 DATE\n",
      "two nights 612 622 DATE\n",
      "Agnes 627 632 PERSON\n",
      "Hotel/Resort 779 791 FAC\n",
      "TEHP/18/12809 836 849 ORG\n",
      "Arun Gadamshetty 857 873 PERSON\n",
      "153021 882 888 CARDINAL\n",
      "Department : Spares Manufacturing Department 889 933 ORG\n",
      "Hotel/Resort 1025 1037 FAC\n",
      "TEHP/18/12809 1082 1095 ORG\n",
      "Arun Gadamshetty 1103 1119 PERSON\n",
      "153021 1128 1134 CARDINAL\n",
      "Department : Spares Manufacturing Department 1135 1179 ORG\n",
      "Hotel/Resort 1271 1283 FAC\n",
      "TEHP/18/12809 1328 1341 ORG\n",
      "Arun Gadamshetty 1349 1365 PERSON\n",
      "153021 1374 1380 CARDINAL\n",
      "Department : Spares Manufacturing Department 1381 1425 ORG\n",
      "Hotel/Resort 1517 1529 FAC\n",
      "TEHP/18/12809 1574 1587 ORG\n",
      "Arun Gadamshetty 1595 1611 PERSON\n",
      "153021 1620 1626 CARDINAL\n",
      "Department : Spares Manufacturing Department 1627 1671 ORG\n",
      "Hotel/Resort 1763 1775 FAC\n",
      "TEHP/18/12809 1820 1833 ORG\n",
      "Arun Gadamshetty 1841 1857 PERSON\n",
      "153021 1866 1872 CARDINAL\n",
      "Department : Spares Manufacturing Department 1873 1917 ORG\n",
      "31st December 2018 1976 1994 DATE\n",
      "31/12/2018 2159 2169 DATE\n",
      "1/1/2019 2171 2179 DATE\n",
      "31/12/2018 2208 2218 DATE\n",
      "1/1/2019 2220 2228 DATE\n",
      "lake palace 2254 2265 LOC\n",
      "8th January 2019 2318 2334 DATE\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "multipleLines = \"\"\"i want to book a room at Lake palace.\n",
    "Check in date is - 31/12/2018\n",
    "check out - 1/1/2019\"\"\"\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# doc = trainedModel(u''+entityDf[\"text\"][13])\n",
    "doc = trainedModel(u''+ \" \".join(entityDf[\"text\"]))\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake palace 25 36 LOC\n",
      "31st December 2018 56 74 DATE\n"
     ]
    }
   ],
   "source": [
    "# \" \".join(entityDf[\"text\"])\n",
    "doc = trainedModel(u''+ \"\"\"i want to book a room at Lake palace Check in date is - 31st December 2018 \n",
    "check out - 1st January 2019\"\"\")\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"i want to book a room at Lake palace. Check in date is - 31st December 2018, check out - 1st January 2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 38)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dte1 = \"31/12/2018\"\n",
    "dte2 = \"1/1/2019\"\n",
    "tetx = \"check in and check out date 31/12/2018, 1/1/2019\"\n",
    "tetx.index(dte1), + tetx.index(dte1) + len(dte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 48)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetx.index(dte2), + tetx.index(dte2) + len(dte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 75)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1 =\"31st December 2018\"\n",
    "loc = \"Lake palace\"\n",
    "date2 =\"1st January 2019\"\n",
    "sentence[0].index(date1), + sentence[0].index(date1) + len(date1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 105)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0].index(date2), + sentence[0].index(date2) + len(date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to book a room at Lake palace. Check in date is - 31st December 2018, check out - 1st January 2019'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityDf[\"text\"][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E001] No component 'textcat' found in pipeline. Available names: ['tagger', 'parser', 'ner']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-88d2a8cb82b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'textcat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mget_pipe\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpipe_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E001] No component 'textcat' found in pipeline. Available names: ['tagger', 'parser', 'ner']\""
     ]
    }
   ],
   "source": [
    "trainedModel.get_pipe('textcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "palace = nlp.vocab[u'palace']\n",
    "Palace = nlp.vocab[u'Palace']\n",
    "palace.is_lower = Palace.is_lower\n",
    "palace.shape = Palace.shape\n",
    "palace.is_upper = Palace.is_upper\n",
    "palace.cluster = Palace.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/12/2018 28 38 DATE\n",
      "1/1/2019 40 48 DATE\n"
     ]
    }
   ],
   "source": [
    "### Testing\n",
    "\n",
    "doc = trainedModel(u''+ \"check in and check out date 31/12/2018, 1/1/2019\")\n",
    "trn_data = []\n",
    "for ent in doc.ents:\n",
    "#     trn_data.append({Entity, end_char,label,start_char, text})\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lexeme.Lexeme at 0x18b175181f8>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-34bc8664a73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'palace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "nlp(u'palace').ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south JJ \n",
      "korea NN GPE\n",
      "is VBZ \n",
      "a DT \n",
      "state NN \n",
      "in IN \n",
      "asia NN \n",
      ". . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'south korea is a state in asia.')\n",
    "for word in doc:\n",
    "    print(word.text, word.tag_, word.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {w.prob: w.orth for w in nlp.vocab}\n",
    "usually_titled = [w for w in nlp.vocab if w.is_title and probs.get(w.lower, -10000) < probs.get(w.orth, -10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usually_titled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import ORTH, LEMMA, TAG, ENT_TYPE, ENT_IOB\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('south', 'GPE'), ('korea', 'GPE')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case('south korea', [{ORTH: 'south korea', LEMMA: 'South korea', TAG: 'NNP', ENT_TYPE: 'GPE', ENT_IOB: 3}])\n",
    "doc = nlp(u'there are many innovative companies in south korea.')\n",
    "[(w.text, w.label_) for w in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 'EX', ''),\n",
       " ('are', 'VBP', ''),\n",
       " ('many', 'JJ', ''),\n",
       " ('innovative', 'JJ', ''),\n",
       " ('companies', 'NNS', ''),\n",
       " ('in', 'IN', ''),\n",
       " ('india', 'NNP', 'GPE'),\n",
       " ('.', '.', '')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case('india', [{ORTH: 'india', LEMMA: 'India', TAG: 'NNP', ENT_TYPE: 'GPE', ENT_IOB: 3}])\n",
    "doc = nlp(u'there are many innovative companies in india.')\n",
    "[(w.text, w.tag_, w.ent_type_) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korea GPE\n",
      "asia GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'south korea is a state in asia.')\n",
    "for word in doc.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../datawe/raw/Email_Classification/small_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taj', 'FAC'), ('lake', 'FAC'), ('pallace', 'FAC')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.attrs import ORTH, LEMMA, TAG, ENT_TYPE, ENT_IOB\n",
    "# nlp = spacy.load('en')\n",
    "nlp.tokenizer.add_special_case('taj', [{ORTH: 'taj', LEMMA: 'Taj', TAG: 'NNP', ENT_TYPE: 'FAC', ENT_IOB: 3}])\n",
    "nlp.tokenizer.add_special_case('lake', [{ORTH: 'lake', LEMMA: 'Lake', TAG: 'NNP', ENT_TYPE: 'FAC', ENT_IOB: 3}])\n",
    "nlp.tokenizer.add_special_case('pallace', [{ORTH: 'pallace', LEMMA: 'Pallace', TAG: 'NNP', ENT_TYPE: 'FAC', ENT_IOB: 3}])\n",
    "\n",
    "doc = nlp(u'there are many innovative companies in taj lake pallace .')\n",
    "[(w.text, w.label_) for w in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
