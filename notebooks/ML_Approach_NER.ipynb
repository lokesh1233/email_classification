{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sent: ['Linux', 'is', 'the', 'best', 'OS']\n",
    "# Labels: ['OS','IR','IR','IR','IR']\n",
    " \n",
    "# Sent_s: ['Ubuntu', 'is', 'my', 'favorite', 'OS']\n",
    "# Labels_l: ['OS','IR','IR','IR','IR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Linux', 'OS'), ('is', 'IR'), ('the', 'IR'), ('best', 'IR'), ('OS', 'IR')], [('Ubuntu', 'OS'), ('is', 'IR'), ('my', 'IR'), ('favourite', 'IR'), ('OS', 'IR')], [('lokesh', 'Person'), ('is', 'IR'), ('my', 'IR'), ('name', 'IR')], [('lokesh', 'Person'), ('and', 'IR'), ('rajesh', 'Person'), ('are', 'IR'), ('my', 'IR'), ('friends', 'IR')], [('my', 'IR'), ('name', 'IR'), ('is', 'IR'), ('lokesh', 'Person')]]\n"
     ]
    }
   ],
   "source": [
    "data = [(['Linux', 'is', 'the', 'best', 'OS'], ['OS','IR','IR','IR','IR']),\n",
    "(['Ubuntu', 'is', 'my', 'favourite', 'OS'], ['OS','IR','IR','IR','IR']),\n",
    "       (['lokesh', 'is', 'my', 'name'], ['Person','IR','IR','IR']),\n",
    "        (['lokesh', 'and', 'rajesh', 'are', 'my', 'friends'], ['Person','IR','Person','IR','IR','IR']),\n",
    "       (['my', 'name', 'is', 'lokesh'], ['IR','IR','IR','Person'])]\n",
    "corpus = []\n",
    "for (doc, tags) in data:\n",
    "    doc_tag = []\n",
    "    for word, tag in zip(doc,tags):\n",
    "        doc_tag.append((word, tag))\n",
    "    corpus.append(doc_tag)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'word.word': 'Linux', 'BOS': True, 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'Linux', 'word.nextword': 'the'}, {'word.word': 'the', 'word.prevword': 'is', 'word.nextword': 'best'}, {'word.word': 'best', 'word.prevword': 'the', 'word.nextword': 'OS'}, {'word.word': 'OS', 'word.prevword': 'best', 'EOS': True}], [{'word.word': 'Ubuntu', 'BOS': True, 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'Ubuntu', 'word.nextword': 'my'}, {'word.word': 'my', 'word.prevword': 'is', 'word.nextword': 'favourite'}, {'word.word': 'favourite', 'word.prevword': 'my', 'word.nextword': 'OS'}, {'word.word': 'OS', 'word.prevword': 'favourite', 'EOS': True}], [{'word.word': 'lokesh', 'BOS': True, 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'lokesh', 'word.nextword': 'my'}, {'word.word': 'my', 'word.prevword': 'is', 'word.nextword': 'name'}, {'word.word': 'name', 'word.prevword': 'my', 'EOS': True}], [{'word.word': 'lokesh', 'BOS': True, 'word.nextword': 'and'}, {'word.word': 'and', 'word.prevword': 'lokesh', 'word.nextword': 'rajesh'}, {'word.word': 'rajesh', 'word.prevword': 'and', 'word.nextword': 'are'}, {'word.word': 'are', 'word.prevword': 'rajesh', 'word.nextword': 'my'}, {'word.word': 'my', 'word.prevword': 'are', 'word.nextword': 'friends'}, {'word.word': 'friends', 'word.prevword': 'my', 'EOS': True}], [{'word.word': 'my', 'BOS': True, 'word.nextword': 'name'}, {'word.word': 'name', 'word.prevword': 'my', 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'name', 'word.nextword': 'lokesh'}, {'word.word': 'lokesh', 'word.prevword': 'is', 'EOS': True}]]\n"
     ]
    }
   ],
   "source": [
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "#     print(i)\n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "    }\n",
    "    # Features from previous word\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        features['word.prevword'] = prevword\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "        \n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        features['word.nextword'] = nextword\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    " \n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    " \n",
    "X = [extract_features(doc) for doc in corpus]\n",
    "print(X)\n",
    "# X = [[{'BOS': True, 'word.word': 'Linux', 'word.nextword': 'is'},\n",
    "# {'word.nextword': 'the', 'word.word': 'is', 'word.prevword': 'Linux'},\n",
    "# {'word.nextword': 'best', 'word.word': 'the', 'word.prevword': 'is'},\n",
    "# {'word.nextword': 'OS', 'word.word': 'best', 'word.prevword': 'the'},\n",
    "# {'EOS': True, 'word.word': 'OS', 'word.prevword': 'best'}],\n",
    "# [{'BOS': True, 'word.word': 'Ubuntu', 'word.nextword': 'is'},\n",
    "# {'word.nextword': 'my', 'word.word': 'is', 'word.prevword': 'Ubuntu'},\n",
    "# {'word.nextword': 'favorite', 'word.word': 'my', 'word.prevword': 'is'},\n",
    "# {'word.nextword': 'OS', 'word.word': 'favorite', 'word.prevword': 'my'},\n",
    "# {'EOS': True, 'word.word': 'OS', 'word.prevword': 'favourite'}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OS', 'IR', 'IR', 'IR', 'IR'], ['OS', 'IR', 'IR', 'IR', 'IR'], ['Person', 'IR', 'IR', 'IR'], ['Person', 'IR', 'Person', 'IR', 'IR', 'IR'], ['IR', 'IR', 'IR', 'Person']]\n"
     ]
    }
   ],
   "source": [
    "def get_labels(doc):\n",
    "    return [tag for (token,tag) in doc]\n",
    "y = [get_labels(doc) for doc in corpus]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite.estimator import CRF\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=False, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=20,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'OS': 0.29201078186845686, 'IR': 0.14427271076355122, 'Person': 0.5637165073679918}]\n"
     ]
    }
   ],
   "source": [
    "test = [['rajesh', 'is', 'my', 'name']]\n",
    "X_test = extract_features(test)\n",
    "print(crf.predict_marginals_single(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CRF in module sklearn_crfsuite.estimator object:\n",
      "\n",
      "class CRF(sklearn.base.BaseEstimator)\n",
      " |  python-crfsuite wrapper with interface siimlar to scikit-learn.\n",
      " |  It allows to use a familiar fit/predict interface and scikit-learn\n",
      " |  model selection utilities (cross-validation, hyperparameter optimization).\n",
      " |  \n",
      " |  Unlike pycrfsuite.Trainer / pycrfsuite.Tagger this object is picklable;\n",
      " |  on-disk files are managed automatically.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  algorithm : str, optional (default='lbfgs')\n",
      " |      Training algorithm. Allowed values:\n",
      " |  \n",
      " |      * ``'lbfgs'`` - Gradient descent using the L-BFGS method\n",
      " |      * ``'l2sgd'`` - Stochastic Gradient Descent with L2 regularization term\n",
      " |      * ``'ap'`` - Averaged Perceptron\n",
      " |      * ``'pa'`` - Passive Aggressive (PA)\n",
      " |      * ``'arow'`` - Adaptive Regularization Of Weight Vector (AROW)\n",
      " |  \n",
      " |  min_freq : float, optional (default=0)\n",
      " |      Cut-off threshold for occurrence\n",
      " |      frequency of a feature. CRFsuite will ignore features whose\n",
      " |      frequencies of occurrences in the training data are no greater\n",
      " |      than `min_freq`. The default is no cut-off.\n",
      " |  \n",
      " |  all_possible_states : bool, optional (default=False)\n",
      " |      Specify whether CRFsuite generates state features that do not even\n",
      " |      occur in the training data (i.e., negative state features).\n",
      " |      When True, CRFsuite generates state features that associate all of\n",
      " |      possible combinations between attributes and labels.\n",
      " |  \n",
      " |      Suppose that the numbers of attributes and labels are A and L\n",
      " |      respectively, this function will generate (A * L) features.\n",
      " |      Enabling this function may improve the labeling accuracy because\n",
      " |      the CRF model can learn the condition where an item is not predicted\n",
      " |      to its reference label. However, this function may also increase\n",
      " |      the number of features and slow down the training process\n",
      " |      drastically. This function is disabled by default.\n",
      " |  \n",
      " |  all_possible_transitions : bool, optional (default=False)\n",
      " |      Specify whether CRFsuite generates transition features that\n",
      " |      do not even occur in the training data (i.e., negative transition\n",
      " |      features). When True, CRFsuite generates transition features that\n",
      " |      associate all of possible label pairs. Suppose that the number\n",
      " |      of labels in the training data is L, this function will\n",
      " |      generate (L * L) transition features.\n",
      " |      This function is disabled by default.\n",
      " |  \n",
      " |  c1 : float, optional (default=0)\n",
      " |      The coefficient for L1 regularization.\n",
      " |      If a non-zero value is specified, CRFsuite switches to the\n",
      " |      Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) method.\n",
      " |      The default value is zero (no L1 regularization).\n",
      " |  \n",
      " |      Supported training algorithms: lbfgs\n",
      " |  \n",
      " |  c2 : float, optional (default=1.0)\n",
      " |      The coefficient for L2 regularization.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd, lbfgs\n",
      " |  \n",
      " |  max_iterations : int, optional (default=None)\n",
      " |      The maximum number of iterations for optimization algorithms.\n",
      " |      Default value depends on training algorithm:\n",
      " |  \n",
      " |      * lbfgs - unlimited;\n",
      " |      * l2sgd - 1000;\n",
      " |      * ap - 100;\n",
      " |      * pa - 100;\n",
      " |      * arow - 100.\n",
      " |  \n",
      " |  num_memories : int, optional (default=6)\n",
      " |      The number of limited memories for approximating the inverse hessian\n",
      " |      matrix.\n",
      " |  \n",
      " |      Supported training algorithms: lbfgs\n",
      " |  \n",
      " |  epsilon : float, optional (default=1e-5)\n",
      " |      The epsilon parameter that determines the condition of convergence.\n",
      " |  \n",
      " |      Supported training algorithms: ap, arow, lbfgs, pa\n",
      " |  \n",
      " |  period : int, optional (default=10)\n",
      " |      The duration of iterations to test the stopping criterion.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd, lbfgs\n",
      " |  \n",
      " |  delta : float, optional (default=1e-5)\n",
      " |      The threshold for the stopping criterion; an iteration stops\n",
      " |      when the improvement of the log likelihood over the last\n",
      " |      `period` iterations is no greater than this threshold.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd, lbfgs\n",
      " |  \n",
      " |  linesearch : str, optional (default='MoreThuente')\n",
      " |      The line search algorithm used in L-BFGS updates. Allowed values:\n",
      " |  \n",
      " |      * ``'MoreThuente'`` - More and Thuente's method;\n",
      " |      * ``'Backtracking'`` - backtracking method with regular Wolfe condition;\n",
      " |      * ``'StrongBacktracking'`` -  backtracking method with strong Wolfe\n",
      " |        condition.\n",
      " |  \n",
      " |      Supported training algorithms: lbfgs\n",
      " |  \n",
      " |  max_linesearch : int, optional (default=20)\n",
      " |      The maximum number of trials for the line search algorithm.\n",
      " |  \n",
      " |      Supported training algorithms: lbfgs\n",
      " |  \n",
      " |  calibration_eta : float, optional (default=0.1)\n",
      " |      The initial value of learning rate (eta) used for calibration.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd\n",
      " |  \n",
      " |  calibration_rate : float, optional (default=2.0)\n",
      " |      The rate of increase/decrease of learning rate for calibration.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd\n",
      " |  \n",
      " |  calibration_samples : int, optional (default=1000)\n",
      " |      The number of instances used for calibration.\n",
      " |      The calibration routine randomly chooses instances no larger\n",
      " |      than `calibration_samples`.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd\n",
      " |  \n",
      " |  calibration_candidates : int, optional (default=10)\n",
      " |      The number of candidates of learning rate.\n",
      " |      The calibration routine terminates after finding\n",
      " |      `calibration_samples` candidates of learning rates\n",
      " |      that can increase log-likelihood.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd\n",
      " |  \n",
      " |  calibration_max_trials : int, optional (default=20)\n",
      " |      The maximum number of trials of learning rates for calibration.\n",
      " |      The calibration routine terminates after trying\n",
      " |      `calibration_max_trials` candidate values of learning rates.\n",
      " |  \n",
      " |      Supported training algorithms: l2sgd\n",
      " |  \n",
      " |  pa_type : int, optional (default=1)\n",
      " |      The strategy for updating feature weights. Allowed values:\n",
      " |  \n",
      " |      * 0 - PA without slack variables;\n",
      " |      * 1 - PA type I;\n",
      " |      * 2 - PA type II.\n",
      " |  \n",
      " |      Supported training algorithms: pa\n",
      " |  \n",
      " |  c : float, optional (default=1)\n",
      " |      Aggressiveness parameter (used only for PA-I and PA-II).\n",
      " |      This parameter controls the influence of the slack term on the\n",
      " |      objective function.\n",
      " |  \n",
      " |      Supported training algorithms: pa\n",
      " |  \n",
      " |  error_sensitive : bool, optional (default=True)\n",
      " |      If this parameter is True, the optimization routine includes\n",
      " |      into the objective function the square root of the number of\n",
      " |      incorrect labels predicted by the model.\n",
      " |  \n",
      " |      Supported training algorithms: pa\n",
      " |  \n",
      " |  averaging : bool, optional (default=True)\n",
      " |      If this parameter is True, the optimization routine computes\n",
      " |      the average of feature weights at all updates in the training\n",
      " |      process (similarly to Averaged Perceptron).\n",
      " |  \n",
      " |      Supported training algorithms: pa\n",
      " |  \n",
      " |  variance : float, optional (default=1)\n",
      " |      The initial variance of every feature weight.\n",
      " |      The algorithm initialize a vector of feature weights as\n",
      " |      a multivariate Gaussian distribution with mean 0\n",
      " |      and variance `variance`.\n",
      " |  \n",
      " |      Supported training algorithms: arow\n",
      " |  \n",
      " |  gamma : float, optional (default=1)\n",
      " |      The tradeoff between loss function and changes of feature weights.\n",
      " |  \n",
      " |      Supported training algorithms: arow\n",
      " |  \n",
      " |  verbose : bool, optional (default=False)\n",
      " |      Enable trainer verbose mode.\n",
      " |  \n",
      " |  model_filename : str, optional (default=None)\n",
      " |      A path to an existing CRFSuite model.\n",
      " |      This parameter allows to load and use existing crfsuite models.\n",
      " |  \n",
      " |      By default, model files are created automatically and saved\n",
      " |      in temporary locations; the preferred way to save/load CRF models\n",
      " |      is to use pickle (or its alternatives like joblib).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CRF\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, algorithm=None, min_freq=None, all_possible_states=None, all_possible_transitions=None, c1=None, c2=None, max_iterations=None, num_memories=None, epsilon=None, period=None, delta=None, linesearch=None, max_linesearch=None, calibration_eta=None, calibration_rate=None, calibration_samples=None, calibration_candidates=None, calibration_max_trials=None, pa_type=None, c=None, error_sensitive=None, averaging=None, variance=None, gamma=None, verbose=False, model_filename=None, keep_tempfiles=False, trainer_cls=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, X_dev=None, y_dev=None)\n",
      " |      Train a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : list of lists of dicts\n",
      " |          Feature dicts for several documents (in a python-crfsuite format).\n",
      " |      \n",
      " |      y : list of lists of strings\n",
      " |          Labels for several documents.\n",
      " |      \n",
      " |      X_dev : (optional) list of lists of dicts\n",
      " |          Feature dicts used for testing.\n",
      " |      \n",
      " |      y_dev : (optional) list of lists of strings\n",
      " |          Labels corresponding to X_dev.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Make a prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : list of lists of dicts\n",
      " |          feature dicts in python-crfsuite format\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of lists of strings\n",
      " |          predicted labels\n",
      " |  \n",
      " |  predict_marginals(self, X)\n",
      " |      Make a prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : list of lists of dicts\n",
      " |          feature dicts in python-crfsuite format\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of lists of dicts\n",
      " |          predicted probabilities for each label at each position\n",
      " |  \n",
      " |  predict_marginals_single(self, xseq)\n",
      " |      Make a prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xseq : list of dicts\n",
      " |          feature dicts in python-crfsuite format\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of dicts\n",
      " |          predicted probabilities for each label at each position\n",
      " |  \n",
      " |  predict_single(self, xseq)\n",
      " |      Make a prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xseq : list of dicts\n",
      " |          feature dicts in python-crfsuite format\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of strings\n",
      " |          predicted labels\n",
      " |  \n",
      " |  score(self, X, y)\n",
      " |      Return accuracy score computed for sequence items.\n",
      " |      \n",
      " |      For other metrics check :mod:`sklearn_crfsuite.metrics`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  attributes_\n",
      " |      A list of known attributes.\n",
      " |  \n",
      " |  classes_\n",
      " |      A list of class labels.\n",
      " |  \n",
      " |  num_attributes_\n",
      " |      Number of non-zero CRF attributes.\n",
      " |  \n",
      " |  size_\n",
      " |      Size of the CRF model, in bytes.\n",
      " |  \n",
      " |  state_features_\n",
      " |      Dict with state feature coefficients:\n",
      " |      ``{(attr_name, label): coef}``\n",
      " |  \n",
      " |  tagger_\n",
      " |      pycrfsuite.Tagger instance.\n",
      " |  \n",
      " |  transition_features_\n",
      " |      Dict with transition feature coefficients:\n",
      " |      ``{(label_from, label_to): coef}``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OS', 'IR', 'Person']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nltk.tag.stanford import NERTagger\n",
    "# import nltk.tag.stanford as st\n",
    "# import os\n",
    "# java_path = \"/Java/jdk1.8.0_45/bin/java.exe\"\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "# st = st.StanfordNERTagger('../ner-model.ser.gz','../stanford-ner.jar')\n",
    "# tagging = st.tag(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "import random\n",
    " \n",
    "conll_data = list(conll2000.chunked_sents())\n",
    "random.shuffle(conll_data)\n",
    "train_sents = conll_data[:int(len(conll_data) * 0.8)]\n",
    "test_sents = conll_data[int(len(conll_data) * 0.8 + 1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
