{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(SOME_LONG_TEXT_HERE)\n",
    "# ents = [ent for ent in list(doc.ents) if ent.label_ != 'DATE']\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the email modules we'll need\n",
    "import glob\n",
    "import email\n",
    "import mailparser\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "path = '../datawe/raw/Email_Classification/*'\n",
    "email_types = glob.glob(path)\n",
    "appendFilesData = []\n",
    "file_raw_data = [] \n",
    "for folder in email_types:\n",
    "    files = glob.glob(folder+\"/*.txt\")\n",
    "    email_type = folder.split('\\\\')[1]\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name) as fp:\n",
    "                raw_data = fp.read()\n",
    "                file_raw_data.append(raw_data)\n",
    "                msg = mailparser.parse_from_string(raw_data)\n",
    "                appendFilesData.append({\n",
    "                    \"to\":msg.to,\n",
    "                    \"from\":msg.from_,\n",
    "                    \"subject\":msg.subject,\n",
    "                    \"date\":msg.date,\n",
    "#                     \"sent\":msg[\"Sent\"],\n",
    "#                     \"importance\":msg[\"Importance\"],\n",
    "                    \"content\": raw_data, #msg.body,\n",
    "                    \"class_to_exec\":email_type,\n",
    "                })\n",
    "         \n",
    "        except IOError as exc:\n",
    "            print('Exception')\n",
    "\n",
    "            \n",
    "#creating pandas dataframe\n",
    "data = pd.DataFrame(appendFilesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##text cleaning\n",
    "def clean(doc):\n",
    "    words_to_exclude = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation)\n",
    "    \n",
    "#     doc = doc.replace('\\n', ' ')\n",
    "    tagged_sentence = nltk.tag.pos_tag(doc.split())\n",
    "    edited_sentence = ' '.join([word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS'])\n",
    "\n",
    "    word_free = \" \".join([i for i in edited_sentence.lower().split() if i not in words_to_exclude])\n",
    "    punc_free = ''.join(ch for ch in word_free if ch not in exclude)\n",
    "    alpha_free = \" \".join(word for word in punc_free.split() if word.isalpha())\n",
    "    stemmer = SnowballStemmer(\"english\").stem\n",
    "    stem_free = \" \".join(stemmer(stem) for stem in alpha_free.split())\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in stem_free.split())\n",
    "    \n",
    "#     print(edited_sentence)\n",
    "#     print(word_free)\n",
    "#     print(punc_free)\n",
    "#     print(alpha_free)\n",
    "#     print(stem_free)\n",
    "    \n",
    "\n",
    "    return normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text transform \n",
    "textFeatures = data['content'].copy()\n",
    "textFeatures = textFeatures.apply(clean)\n",
    "vectorizer =  TfidfVectorizer()#TfidfVectorizer(\"english\") \n",
    "features = vectorizer.fit_transform(textFeatures)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, data['class_to_exec'], test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "mnb.fit(features_train, labels_train)\n",
    "prediction = mnb.predict(features_test)\n",
    "accuracy_score(labels_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:626: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## grid serach CV for tuning hyperparameters\n",
    "import sklearn.model_selection as modsel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "tfidf_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer.vocabulary_\n",
    "\n",
    "pre = tfidf_search.predict(features_test)\n",
    "accuracy_score(labels_test,pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
